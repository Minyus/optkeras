{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "adlTq181lzrT"
   },
   "source": [
    "This notebook demonstrates how to use OptKeras, a Python package to optimize hyperparameters of Keras Deep Learning Models using Optuna.\n",
    "\n",
    "Please see the GitHub repository of OptKeras for details:\n",
    "https://github.com/Minyus/optkeras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run in non-Colab environment.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google import colab\n",
    "    !pip3 install optuna==0.14.0\n",
    "    !pip3 install optkeras==0.0.7\n",
    "    # Alternatively you can install from the GitHub repository\n",
    "    # !pip install git+https://github.com/Minyus/optkeras.git\n",
    "except:\n",
    "    print('Run in non-Colab environment.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0nikvt-fppC8"
   },
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "i85DbXl0nvmU",
    "outputId": "d08b0720-5b6b-4907-f073-2410d64c0c48"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras 2.2.4\n",
      "TensorFlow 1.14.0\n",
      "Optuna 0.14.0\n",
      "OptKeras 0.0.7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Flatten, Dense, Conv2D\n",
    "from keras.layers import MaxPooling2D, Dropout, BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD, Adagrad, RMSprop, Adam, Adadelta, Adamax, Nadam\n",
    "import keras.backend as K\n",
    "\n",
    "import keras\n",
    "print('Keras', keras.__version__)\n",
    "\n",
    "%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "print('TensorFlow', tf.__version__)\n",
    "\n",
    "# import Optuna and OptKeras after Keras\n",
    "import optuna \n",
    "print('Optuna', optuna.__version__)\n",
    "\n",
    "from optkeras.optkeras import OptKeras\n",
    "import optkeras\n",
    "print('OptKeras', optkeras.__version__)\n",
    "\n",
    "# (Optional) Disable messages from Optuna below WARN level.\n",
    "optuna.logging.set_verbosity(optuna.logging.WARN) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mq16j0E4olJO"
   },
   "source": [
    "## Set up Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "sfMZGHCO4DrV",
    "outputId": "a6654e63-6e1d-4d2b-e5da-d655db39af39"
   },
   "outputs": [],
   "source": [
    "dataset_name = 'MNIST'\n",
    "\n",
    "if dataset_name in ['MNIST']:\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    img_x, img_y = x_train.shape[1], x_train.shape[2]\n",
    "    x_train = x_train.reshape(-1, img_x, img_y, 1)\n",
    "    x_test = x_test.reshape(-1, img_x, img_y, 1)   \n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    num_classes = 10\n",
    "    input_shape = (img_x, img_y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "D6QEQ_Dk6ReL",
    "outputId": "41e419fd-9622-4982-8cbb-9ce601bcf396"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:  (60000, 28, 28, 1)\n",
      "y_train (60000,)\n",
      "x_test:  (10000, 28, 28, 1)\n",
      "y_test (10000,)\n",
      "input_shape:  (28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print('x_train: ', x_train.shape)\n",
    "print('y_train', y_train.shape)\n",
    "print('x_test: ', x_test.shape)\n",
    "print('y_test', y_test.shape)\n",
    "print('input_shape: ', input_shape )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Wb1ZX2vJDO1"
   },
   "source": [
    "## A simple Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "m5NqIOlKJBpO",
    "outputId": "2e4e1d33-1792-4c5d-e897-c0f9fbb78813"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0801 15:40:08.024789 139680493655872 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0801 15:40:08.071118 139680493655872 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0801 15:40:08.076984 139680493655872 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0801 15:40:08.142308 139680493655872 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0801 15:40:08.153276 139680493655872 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0801 15:40:08.274288 139680493655872 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0801 15:40:08.316937 139680493655872 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - ETA: 38s - loss: 2.3035 - acc: 0.08 - ETA: 25s - loss: 2.2453 - acc: 0.21 - ETA: 22s - loss: 2.1728 - acc: 0.32 - ETA: 20s - loss: 2.0984 - acc: 0.39 - ETA: 19s - loss: 2.0308 - acc: 0.45 - ETA: 18s - loss: 1.9556 - acc: 0.49 - ETA: 18s - loss: 1.8807 - acc: 0.53 - ETA: 17s - loss: 1.8122 - acc: 0.56 - ETA: 17s - loss: 1.7424 - acc: 0.59 - ETA: 17s - loss: 1.6749 - acc: 0.61 - ETA: 16s - loss: 1.6128 - acc: 0.63 - ETA: 16s - loss: 1.5493 - acc: 0.65 - ETA: 16s - loss: 1.4956 - acc: 0.66 - ETA: 16s - loss: 1.4389 - acc: 0.67 - ETA: 16s - loss: 1.3911 - acc: 0.68 - ETA: 15s - loss: 1.3462 - acc: 0.69 - ETA: 15s - loss: 1.3033 - acc: 0.70 - ETA: 15s - loss: 1.2627 - acc: 0.71 - ETA: 15s - loss: 1.2280 - acc: 0.72 - ETA: 15s - loss: 1.1934 - acc: 0.72 - ETA: 15s - loss: 1.1625 - acc: 0.73 - ETA: 14s - loss: 1.1300 - acc: 0.73 - ETA: 14s - loss: 1.1002 - acc: 0.74 - ETA: 14s - loss: 1.0758 - acc: 0.74 - ETA: 14s - loss: 1.0515 - acc: 0.75 - ETA: 14s - loss: 1.0292 - acc: 0.75 - ETA: 13s - loss: 1.0073 - acc: 0.76 - ETA: 13s - loss: 0.9855 - acc: 0.76 - ETA: 13s - loss: 0.9642 - acc: 0.77 - ETA: 13s - loss: 0.9452 - acc: 0.77 - ETA: 13s - loss: 0.9305 - acc: 0.77 - ETA: 13s - loss: 0.9149 - acc: 0.78 - ETA: 12s - loss: 0.8987 - acc: 0.78 - ETA: 12s - loss: 0.8830 - acc: 0.78 - ETA: 12s - loss: 0.8681 - acc: 0.78 - ETA: 12s - loss: 0.8545 - acc: 0.79 - ETA: 12s - loss: 0.8400 - acc: 0.79 - ETA: 12s - loss: 0.8259 - acc: 0.79 - ETA: 11s - loss: 0.8142 - acc: 0.80 - ETA: 11s - loss: 0.8046 - acc: 0.80 - ETA: 11s - loss: 0.7942 - acc: 0.80 - ETA: 11s - loss: 0.7839 - acc: 0.80 - ETA: 11s - loss: 0.7730 - acc: 0.80 - ETA: 11s - loss: 0.7624 - acc: 0.81 - ETA: 11s - loss: 0.7528 - acc: 0.81 - ETA: 10s - loss: 0.7453 - acc: 0.81 - ETA: 10s - loss: 0.7360 - acc: 0.81 - ETA: 10s - loss: 0.7276 - acc: 0.81 - ETA: 10s - loss: 0.7190 - acc: 0.82 - ETA: 10s - loss: 0.7107 - acc: 0.82 - ETA: 10s - loss: 0.7031 - acc: 0.82 - ETA: 9s - loss: 0.6967 - acc: 0.8262 - ETA: 9s - loss: 0.6892 - acc: 0.827 - ETA: 9s - loss: 0.6822 - acc: 0.829 - ETA: 9s - loss: 0.6763 - acc: 0.830 - ETA: 9s - loss: 0.6696 - acc: 0.831 - ETA: 9s - loss: 0.6646 - acc: 0.832 - ETA: 9s - loss: 0.6590 - acc: 0.834 - ETA: 8s - loss: 0.6538 - acc: 0.835 - ETA: 8s - loss: 0.6480 - acc: 0.836 - ETA: 8s - loss: 0.6427 - acc: 0.837 - ETA: 8s - loss: 0.6383 - acc: 0.838 - ETA: 8s - loss: 0.6328 - acc: 0.839 - ETA: 8s - loss: 0.6273 - acc: 0.840 - ETA: 7s - loss: 0.6223 - acc: 0.841 - ETA: 7s - loss: 0.6183 - acc: 0.842 - ETA: 7s - loss: 0.6133 - acc: 0.843 - ETA: 7s - loss: 0.6092 - acc: 0.844 - ETA: 7s - loss: 0.6046 - acc: 0.845 - ETA: 7s - loss: 0.6005 - acc: 0.846 - ETA: 7s - loss: 0.5963 - acc: 0.847 - ETA: 6s - loss: 0.5917 - acc: 0.848 - ETA: 6s - loss: 0.5882 - acc: 0.848 - ETA: 6s - loss: 0.5849 - acc: 0.849 - ETA: 6s - loss: 0.5809 - acc: 0.850 - ETA: 6s - loss: 0.5764 - acc: 0.851 - ETA: 6s - loss: 0.5722 - acc: 0.852 - ETA: 5s - loss: 0.5680 - acc: 0.853 - ETA: 5s - loss: 0.5639 - acc: 0.854 - ETA: 5s - loss: 0.5595 - acc: 0.855 - ETA: 5s - loss: 0.5557 - acc: 0.856 - ETA: 5s - loss: 0.5528 - acc: 0.857 - ETA: 5s - loss: 0.5500 - acc: 0.858 - ETA: 5s - loss: 0.5466 - acc: 0.859 - ETA: 4s - loss: 0.5427 - acc: 0.859 - ETA: 4s - loss: 0.5395 - acc: 0.860 - ETA: 4s - loss: 0.5359 - acc: 0.861 - ETA: 4s - loss: 0.5334 - acc: 0.862 - ETA: 4s - loss: 0.5303 - acc: 0.862 - ETA: 4s - loss: 0.5276 - acc: 0.863 - ETA: 4s - loss: 0.5245 - acc: 0.863 - ETA: 3s - loss: 0.5218 - acc: 0.864 - ETA: 3s - loss: 0.5189 - acc: 0.865 - ETA: 3s - loss: 0.5159 - acc: 0.866 - ETA: 3s - loss: 0.5130 - acc: 0.866 - ETA: 3s - loss: 0.5109 - acc: 0.867 - ETA: 3s - loss: 0.5083 - acc: 0.867 - ETA: 2s - loss: 0.5056 - acc: 0.868 - ETA: 2s - loss: 0.5028 - acc: 0.868 - ETA: 2s - loss: 0.5005 - acc: 0.869 - ETA: 2s - loss: 0.4983 - acc: 0.870 - ETA: 2s - loss: 0.4956 - acc: 0.870 - ETA: 2s - loss: 0.4937 - acc: 0.870 - ETA: 2s - loss: 0.4915 - acc: 0.871 - ETA: 1s - loss: 0.4895 - acc: 0.872 - ETA: 1s - loss: 0.4872 - acc: 0.872 - ETA: 1s - loss: 0.4855 - acc: 0.872 - ETA: 1s - loss: 0.4831 - acc: 0.873 - ETA: 1s - loss: 0.4807 - acc: 0.874 - ETA: 1s - loss: 0.4785 - acc: 0.874 - ETA: 0s - loss: 0.4764 - acc: 0.875 - ETA: 0s - loss: 0.4743 - acc: 0.875 - ETA: 0s - loss: 0.4720 - acc: 0.876 - ETA: 0s - loss: 0.4695 - acc: 0.876 - ETA: 0s - loss: 0.4673 - acc: 0.877 - ETA: 0s - loss: 0.4650 - acc: 0.878 - ETA: 0s - loss: 0.4631 - acc: 0.878 - 20s 337us/step - loss: 0.4627 - acc: 0.8787 - val_loss: 0.2204 - val_acc: 0.9367\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - ETA: 23s - loss: 0.2267 - acc: 0.93 - ETA: 23s - loss: 0.2275 - acc: 0.94 - ETA: 23s - loss: 0.2277 - acc: 0.94 - ETA: 24s - loss: 0.2313 - acc: 0.94 - ETA: 23s - loss: 0.2148 - acc: 0.94 - ETA: 23s - loss: 0.2153 - acc: 0.94 - ETA: 22s - loss: 0.2135 - acc: 0.94 - ETA: 22s - loss: 0.2146 - acc: 0.94 - ETA: 21s - loss: 0.2189 - acc: 0.94 - ETA: 21s - loss: 0.2191 - acc: 0.93 - ETA: 20s - loss: 0.2179 - acc: 0.93 - ETA: 20s - loss: 0.2264 - acc: 0.93 - ETA: 19s - loss: 0.2265 - acc: 0.93 - ETA: 19s - loss: 0.2252 - acc: 0.93 - ETA: 19s - loss: 0.2213 - acc: 0.93 - ETA: 19s - loss: 0.2211 - acc: 0.93 - ETA: 18s - loss: 0.2207 - acc: 0.93 - ETA: 18s - loss: 0.2173 - acc: 0.93 - ETA: 18s - loss: 0.2205 - acc: 0.93 - ETA: 17s - loss: 0.2220 - acc: 0.93 - ETA: 17s - loss: 0.2227 - acc: 0.93 - ETA: 17s - loss: 0.2235 - acc: 0.93 - ETA: 16s - loss: 0.2252 - acc: 0.93 - ETA: 16s - loss: 0.2233 - acc: 0.93 - ETA: 16s - loss: 0.2247 - acc: 0.93 - ETA: 16s - loss: 0.2245 - acc: 0.93 - ETA: 15s - loss: 0.2246 - acc: 0.93 - ETA: 15s - loss: 0.2250 - acc: 0.93 - ETA: 15s - loss: 0.2261 - acc: 0.93 - ETA: 14s - loss: 0.2241 - acc: 0.93 - ETA: 14s - loss: 0.2219 - acc: 0.93 - ETA: 14s - loss: 0.2216 - acc: 0.93 - ETA: 13s - loss: 0.2210 - acc: 0.93 - ETA: 13s - loss: 0.2204 - acc: 0.93 - ETA: 13s - loss: 0.2196 - acc: 0.93 - ETA: 13s - loss: 0.2188 - acc: 0.93 - ETA: 13s - loss: 0.2177 - acc: 0.93 - ETA: 12s - loss: 0.2167 - acc: 0.93 - ETA: 12s - loss: 0.2170 - acc: 0.93 - ETA: 12s - loss: 0.2163 - acc: 0.93 - ETA: 12s - loss: 0.2153 - acc: 0.93 - ETA: 11s - loss: 0.2157 - acc: 0.93 - ETA: 11s - loss: 0.2143 - acc: 0.93 - ETA: 11s - loss: 0.2132 - acc: 0.93 - ETA: 11s - loss: 0.2120 - acc: 0.93 - ETA: 11s - loss: 0.2112 - acc: 0.93 - ETA: 11s - loss: 0.2111 - acc: 0.93 - ETA: 10s - loss: 0.2108 - acc: 0.93 - ETA: 10s - loss: 0.2093 - acc: 0.94 - ETA: 10s - loss: 0.2091 - acc: 0.94 - ETA: 10s - loss: 0.2090 - acc: 0.94 - ETA: 10s - loss: 0.2089 - acc: 0.94 - ETA: 10s - loss: 0.2083 - acc: 0.94 - ETA: 9s - loss: 0.2082 - acc: 0.9406 - ETA: 9s - loss: 0.2078 - acc: 0.940 - ETA: 9s - loss: 0.2079 - acc: 0.940 - ETA: 9s - loss: 0.2071 - acc: 0.940 - ETA: 9s - loss: 0.2064 - acc: 0.941 - ETA: 9s - loss: 0.2064 - acc: 0.941 - ETA: 9s - loss: 0.2061 - acc: 0.941 - ETA: 8s - loss: 0.2046 - acc: 0.941 - ETA: 8s - loss: 0.2044 - acc: 0.942 - ETA: 8s - loss: 0.2035 - acc: 0.942 - ETA: 8s - loss: 0.2032 - acc: 0.942 - ETA: 8s - loss: 0.2030 - acc: 0.942 - ETA: 8s - loss: 0.2022 - acc: 0.942 - ETA: 7s - loss: 0.2018 - acc: 0.942 - ETA: 7s - loss: 0.2013 - acc: 0.943 - ETA: 7s - loss: 0.2002 - acc: 0.943 - ETA: 7s - loss: 0.1997 - acc: 0.943 - ETA: 7s - loss: 0.1989 - acc: 0.943 - ETA: 7s - loss: 0.1983 - acc: 0.943 - ETA: 7s - loss: 0.1987 - acc: 0.943 - ETA: 7s - loss: 0.1977 - acc: 0.943 - ETA: 7s - loss: 0.1975 - acc: 0.944 - ETA: 6s - loss: 0.1966 - acc: 0.944 - ETA: 6s - loss: 0.1966 - acc: 0.944 - ETA: 6s - loss: 0.1962 - acc: 0.944 - ETA: 6s - loss: 0.1951 - acc: 0.944 - ETA: 6s - loss: 0.1947 - acc: 0.944 - ETA: 6s - loss: 0.1941 - acc: 0.945 - ETA: 5s - loss: 0.1936 - acc: 0.945 - ETA: 5s - loss: 0.1937 - acc: 0.945 - ETA: 5s - loss: 0.1934 - acc: 0.945 - ETA: 5s - loss: 0.1932 - acc: 0.945 - ETA: 5s - loss: 0.1926 - acc: 0.945 - ETA: 4s - loss: 0.1927 - acc: 0.945 - ETA: 4s - loss: 0.1921 - acc: 0.945 - ETA: 4s - loss: 0.1916 - acc: 0.945 - ETA: 4s - loss: 0.1915 - acc: 0.946 - ETA: 4s - loss: 0.1912 - acc: 0.946 - ETA: 4s - loss: 0.1910 - acc: 0.946 - ETA: 3s - loss: 0.1910 - acc: 0.946 - ETA: 3s - loss: 0.1906 - acc: 0.946 - ETA: 3s - loss: 0.1900 - acc: 0.946 - ETA: 3s - loss: 0.1899 - acc: 0.946 - ETA: 3s - loss: 0.1895 - acc: 0.946 - ETA: 3s - loss: 0.1888 - acc: 0.946 - ETA: 2s - loss: 0.1883 - acc: 0.947 - ETA: 2s - loss: 0.1879 - acc: 0.947 - ETA: 2s - loss: 0.1878 - acc: 0.947 - ETA: 2s - loss: 0.1881 - acc: 0.947 - ETA: 2s - loss: 0.1878 - acc: 0.947 - ETA: 2s - loss: 0.1878 - acc: 0.947 - ETA: 1s - loss: 0.1875 - acc: 0.947 - ETA: 1s - loss: 0.1870 - acc: 0.947 - ETA: 1s - loss: 0.1865 - acc: 0.947 - ETA: 1s - loss: 0.1861 - acc: 0.947 - ETA: 1s - loss: 0.1857 - acc: 0.947 - ETA: 1s - loss: 0.1852 - acc: 0.948 - ETA: 0s - loss: 0.1846 - acc: 0.948 - ETA: 0s - loss: 0.1845 - acc: 0.948 - ETA: 0s - loss: 0.1845 - acc: 0.948 - ETA: 0s - loss: 0.1844 - acc: 0.948 - ETA: 0s - loss: 0.1839 - acc: 0.948 - ETA: 0s - loss: 0.1836 - acc: 0.948 - ETA: 0s - loss: 0.1832 - acc: 0.948 - 20s 326us/step - loss: 0.1831 - acc: 0.9487 - val_loss: 0.1411 - val_acc: 0.9615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0997767ac8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(\n",
    "    filters=32,\n",
    "    kernel_size=3,\n",
    "    strides=1,\n",
    "    activation='relu',\n",
    "    input_shape=input_shape ))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(optimizer=Adam(),\n",
    "            loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,\n",
    "          validation_data=(x_test, y_test), shuffle=True,\n",
    "          batch_size=512, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J_MtiEV0kcSd"
   },
   "source": [
    "## Optimization of a simple Keras model without pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1339
    },
    "colab_type": "code",
    "id": "kpr1zgHub1QN",
    "outputId": "2e64cb2e-b2e5-482b-cb88-0b1e01b4e308"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-01 15:40:48.302738] [OptKeras] Ready for optimization. (message printed as verbose is set to 1+)\n",
      "[2019-08-01 15:40:48.489332]  (None) \n",
      "[2019-08-01 15:41:51.471535] Trial#: 0, value: 9.808000e-01| Best trial#: 0, value: 9.808000e-01, params: {'filters': 64, 'kernel_size': 5, 'strides': 1, 'activation': 'relu'}\n",
      "[2019-08-01 15:42:28.326205] Trial#: 1, value: 9.718000e-01| Best trial#: 0, value: 9.808000e-01, params: {'filters': 64, 'kernel_size': 5, 'strides': 1, 'activation': 'relu'}\n",
      "[2019-08-01 15:43:25.273919] Trial#: 2, value: 9.250000e-01| Best trial#: 0, value: 9.808000e-01, params: {'filters': 64, 'kernel_size': 5, 'strides': 1, 'activation': 'relu'}\n",
      "[2019-08-01 15:43:37.728497] Trial#: 3, value: 9.187000e-01| Best trial#: 0, value: 9.808000e-01, params: {'filters': 64, 'kernel_size': 5, 'strides': 1, 'activation': 'relu'}\n",
      "[2019-08-01 15:43:50.947701] Trial#: 4, value: 9.530000e-01| Best trial#: 0, value: 9.808000e-01, params: {'filters': 64, 'kernel_size': 5, 'strides': 1, 'activation': 'relu'}\n"
     ]
    }
   ],
   "source": [
    "study_name = dataset_name + '_Simple'\n",
    "\n",
    "\"\"\" Step 1. Instantiate OptKeras class\n",
    "You can specify arguments for Optuna's create_study method and other arguments \n",
    "for OptKeras such as enable_pruning. \n",
    "\"\"\"\n",
    "\n",
    "ok = OptKeras(study_name=study_name,\n",
    "              monitor='val_acc',\n",
    "              direction='maximize')\n",
    "\n",
    "\n",
    "\"\"\" Step 2. Define objective function for Optuna \"\"\"\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    \"\"\" Step 2.1. Define parameters to try using methods of optuna.trial such as \n",
    "    suggest_categorical. In this simple demo, try 2*2*2*2 = 16 parameter sets: \n",
    "    2 values specified in list for each of 4 parameters \n",
    "    (filters, kernel_size, strides, and activation for convolution).\n",
    "    \"\"\"    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(\n",
    "        filters=trial.suggest_categorical('filters', [32, 64]),\n",
    "        kernel_size=trial.suggest_categorical('kernel_size', [3, 5]),\n",
    "        strides=trial.suggest_categorical('strides', [1, 2]),\n",
    "        activation=trial.suggest_categorical('activation', ['relu', 'linear']),\n",
    "        input_shape=input_shape ))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer=Adam(),\n",
    "                loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    \"\"\" Step 2.2. Specify callbacks(trial) and keras_verbose in fit \n",
    "    (or fit_generator) method of Keras model\n",
    "    \"\"\"\n",
    "    model.fit(x_train, y_train, \n",
    "              validation_data=(x_test, y_test), shuffle=True,\n",
    "              batch_size=512, epochs=2,\n",
    "              callbacks=ok.callbacks(trial),\n",
    "              verbose=ok.keras_verbose )\n",
    "    \n",
    "    \"\"\" Step 2.3. Return trial_best_value (or latest_value) \"\"\"\n",
    "    return ok.trial_best_value\n",
    "\n",
    "\"\"\" Step 3. Run optimize. \n",
    "Set n_trials and/or timeout (in sec) for optimization by Optuna\n",
    "\"\"\"\n",
    "ok.optimize(objective, timeout = 3*60) # Run for 3 minutes for demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1147
    },
    "colab_type": "code",
    "id": "6y5SKUF9qXlp",
    "outputId": "3cbf327c-7078-4c06-a7bb-121dd5a68868"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial number:  0\n",
      "Best value: 0.9808000007629395\n",
      "Best parameters: \n",
      " {'filters': 64, 'kernel_size': 5, 'strides': 1, 'activation': 'relu'}\n",
      "Best parameters (retrieved directly from Optuna) {'filters': 64, 'kernel_size': 5, 'strides': 1, 'activation': 'relu'}\n",
      "Data Frame read from MNIST_Simple_Optuna.csv \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>state</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>activation</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>strides</th>\n",
       "      <th>_Datetime_epoch_begin</th>\n",
       "      <th>_Datetime_epoch_end</th>\n",
       "      <th>_Trial_num</th>\n",
       "      <th>acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>system_attrs__number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>TrialState.COMPLETE</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>2019-08-01 15:40:48.313516</td>\n",
       "      <td>2019-08-01 15:41:51.362694</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-08-01 15:41:19.722212</td>\n",
       "      <td>2019-08-01 15:41:51.198223</td>\n",
       "      <td>0</td>\n",
       "      <td>0.972383</td>\n",
       "      <td>0.101373</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>0.071210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>TrialState.COMPLETE</td>\n",
       "      <td>0.9718</td>\n",
       "      <td>2019-08-01 15:41:51.363248</td>\n",
       "      <td>2019-08-01 15:42:28.214143</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-08-01 15:42:09.938641</td>\n",
       "      <td>2019-08-01 15:42:28.067890</td>\n",
       "      <td>1</td>\n",
       "      <td>0.957200</td>\n",
       "      <td>0.151473</td>\n",
       "      <td>0.9718</td>\n",
       "      <td>0.106681</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>TrialState.COMPLETE</td>\n",
       "      <td>0.9250</td>\n",
       "      <td>2019-08-01 15:42:28.215066</td>\n",
       "      <td>2019-08-01 15:43:25.158421</td>\n",
       "      <td>linear</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-08-01 15:42:56.725928</td>\n",
       "      <td>2019-08-01 15:43:24.981940</td>\n",
       "      <td>2</td>\n",
       "      <td>0.915300</td>\n",
       "      <td>0.295586</td>\n",
       "      <td>0.9250</td>\n",
       "      <td>0.279367</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>TrialState.COMPLETE</td>\n",
       "      <td>0.9187</td>\n",
       "      <td>2019-08-01 15:43:25.159711</td>\n",
       "      <td>2019-08-01 15:43:37.611357</td>\n",
       "      <td>linear</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-08-01 15:43:31.647896</td>\n",
       "      <td>2019-08-01 15:43:37.471011</td>\n",
       "      <td>3</td>\n",
       "      <td>0.913883</td>\n",
       "      <td>0.303336</td>\n",
       "      <td>0.9187</td>\n",
       "      <td>0.281933</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>TrialState.COMPLETE</td>\n",
       "      <td>0.9530</td>\n",
       "      <td>2019-08-01 15:43:37.613132</td>\n",
       "      <td>2019-08-01 15:43:50.925040</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-08-01 15:43:44.556349</td>\n",
       "      <td>2019-08-01 15:43:50.782537</td>\n",
       "      <td>4</td>\n",
       "      <td>0.937700</td>\n",
       "      <td>0.220760</td>\n",
       "      <td>0.9530</td>\n",
       "      <td>0.168444</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number                state   value              datetime_start  \\\n",
       "0       0  TrialState.COMPLETE  0.9808  2019-08-01 15:40:48.313516   \n",
       "1       1  TrialState.COMPLETE  0.9718  2019-08-01 15:41:51.363248   \n",
       "2       2  TrialState.COMPLETE  0.9250  2019-08-01 15:42:28.215066   \n",
       "3       3  TrialState.COMPLETE  0.9187  2019-08-01 15:43:25.159711   \n",
       "4       4  TrialState.COMPLETE  0.9530  2019-08-01 15:43:37.613132   \n",
       "\n",
       "            datetime_complete activation  filters  kernel_size  strides  \\\n",
       "0  2019-08-01 15:41:51.362694       relu       64            5        1   \n",
       "1  2019-08-01 15:42:28.214143       relu       32            5        1   \n",
       "2  2019-08-01 15:43:25.158421     linear       64            5        1   \n",
       "3  2019-08-01 15:43:37.611357     linear       64            5        2   \n",
       "4  2019-08-01 15:43:50.925040       relu       64            5        2   \n",
       "\n",
       "        _Datetime_epoch_begin         _Datetime_epoch_end  _Trial_num  \\\n",
       "0  2019-08-01 15:41:19.722212  2019-08-01 15:41:51.198223           0   \n",
       "1  2019-08-01 15:42:09.938641  2019-08-01 15:42:28.067890           1   \n",
       "2  2019-08-01 15:42:56.725928  2019-08-01 15:43:24.981940           2   \n",
       "3  2019-08-01 15:43:31.647896  2019-08-01 15:43:37.471011           3   \n",
       "4  2019-08-01 15:43:44.556349  2019-08-01 15:43:50.782537           4   \n",
       "\n",
       "        acc      loss  val_acc  val_loss  system_attrs__number  \n",
       "0  0.972383  0.101373   0.9808  0.071210                     0  \n",
       "1  0.957200  0.151473   0.9718  0.106681                     1  \n",
       "2  0.915300  0.295586   0.9250  0.279367                     2  \n",
       "3  0.913883  0.303336   0.9187  0.281933                     3  \n",
       "4  0.937700  0.220760   0.9530  0.168444                     4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Frame read from MNIST_Simple_Keras.csv \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>_Datetime_epoch_begin</th>\n",
       "      <th>_Datetime_epoch_end</th>\n",
       "      <th>_Trial_num</th>\n",
       "      <th>acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-05-10 07:32:28.812548</td>\n",
       "      <td>2019-05-10 07:32:29.642939</td>\n",
       "      <td>0</td>\n",
       "      <td>0.858783</td>\n",
       "      <td>0.532120</td>\n",
       "      <td>0.9315</td>\n",
       "      <td>0.232515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-10 07:32:29.699403</td>\n",
       "      <td>2019-05-10 07:32:30.322589</td>\n",
       "      <td>0</td>\n",
       "      <td>0.946100</td>\n",
       "      <td>0.191616</td>\n",
       "      <td>0.9623</td>\n",
       "      <td>0.141296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-05-10 07:32:30.764731</td>\n",
       "      <td>2019-05-10 07:32:31.711724</td>\n",
       "      <td>1</td>\n",
       "      <td>0.876700</td>\n",
       "      <td>0.438188</td>\n",
       "      <td>0.9179</td>\n",
       "      <td>0.289129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-10 07:32:31.764485</td>\n",
       "      <td>2019-05-10 07:32:32.542477</td>\n",
       "      <td>1</td>\n",
       "      <td>0.918517</td>\n",
       "      <td>0.288177</td>\n",
       "      <td>0.9218</td>\n",
       "      <td>0.280232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-08-01 15:43:25.579514</td>\n",
       "      <td>2019-08-01 15:43:31.557402</td>\n",
       "      <td>3</td>\n",
       "      <td>0.857333</td>\n",
       "      <td>0.532061</td>\n",
       "      <td>0.9161</td>\n",
       "      <td>0.302102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-08-01 15:43:31.647896</td>\n",
       "      <td>2019-08-01 15:43:37.471011</td>\n",
       "      <td>3</td>\n",
       "      <td>0.913883</td>\n",
       "      <td>0.303336</td>\n",
       "      <td>0.9187</td>\n",
       "      <td>0.281933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-08-01 15:43:37.989403</td>\n",
       "      <td>2019-08-01 15:43:44.466711</td>\n",
       "      <td>4</td>\n",
       "      <td>0.858100</td>\n",
       "      <td>0.536923</td>\n",
       "      <td>0.9282</td>\n",
       "      <td>0.252586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-08-01 15:43:44.556349</td>\n",
       "      <td>2019-08-01 15:43:50.782537</td>\n",
       "      <td>4</td>\n",
       "      <td>0.937700</td>\n",
       "      <td>0.220760</td>\n",
       "      <td>0.9530</td>\n",
       "      <td>0.168444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>188 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch       _Datetime_epoch_begin         _Datetime_epoch_end  \\\n",
       "0        0  2019-05-10 07:32:28.812548  2019-05-10 07:32:29.642939   \n",
       "1        1  2019-05-10 07:32:29.699403  2019-05-10 07:32:30.322589   \n",
       "2        0  2019-05-10 07:32:30.764731  2019-05-10 07:32:31.711724   \n",
       "3        1  2019-05-10 07:32:31.764485  2019-05-10 07:32:32.542477   \n",
       "..     ...                         ...                         ...   \n",
       "184      0  2019-08-01 15:43:25.579514  2019-08-01 15:43:31.557402   \n",
       "185      1  2019-08-01 15:43:31.647896  2019-08-01 15:43:37.471011   \n",
       "186      0  2019-08-01 15:43:37.989403  2019-08-01 15:43:44.466711   \n",
       "187      1  2019-08-01 15:43:44.556349  2019-08-01 15:43:50.782537   \n",
       "\n",
       "     _Trial_num       acc      loss  val_acc  val_loss  \n",
       "0             0  0.858783  0.532120   0.9315  0.232515  \n",
       "1             0  0.946100  0.191616   0.9623  0.141296  \n",
       "2             1  0.876700  0.438188   0.9179  0.289129  \n",
       "3             1  0.918517  0.288177   0.9218  0.280232  \n",
       "..          ...       ...       ...      ...       ...  \n",
       "184           3  0.857333  0.532061   0.9161  0.302102  \n",
       "185           3  0.913883  0.303336   0.9187  0.281933  \n",
       "186           4  0.858100  0.536923   0.9282  0.252586  \n",
       "187           4  0.937700  0.220760   0.9530  0.168444  \n",
       "\n",
       "[188 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" Show Results \"\"\"\n",
    "print('Best trial number: ', ok.best_trial.number)\n",
    "print('Best value:', ok.best_trial.value)\n",
    "print('Best parameters: \\n', ok.best_trial.params)\n",
    "\n",
    "\"\"\"\n",
    "Alternatively, you can access Optuna's study object to, for example, \n",
    "get the best parameters as well.\n",
    "Please note that study.best_trial returns error if optimization trials \n",
    "were not completed (e.g. if you interrupt execution) as of Optuna 0.7.0, \n",
    "so usage of OptKeras is recommended.\n",
    "\"\"\"\n",
    "print(\"Best parameters (retrieved directly from Optuna)\", ok.study.best_trial.params)\n",
    "\n",
    "\"\"\" Check the Optuna CSV log file \"\"\"\n",
    "pd.options.display.max_rows = 8 # limit rows to display\n",
    "print('Data Frame read from', ok.optuna_log_file_path, '\\n')\n",
    "display(pd.read_csv(ok.optuna_log_file_path))\n",
    "\n",
    "\"\"\" Check the Keras CSV log file \"\"\"\n",
    "pd.options.display.max_rows = 8 # limit rows to display\n",
    "print('Data Frame read from', ok.keras_log_file_path, '\\n')\n",
    "display(pd.read_csv(ok.keras_log_file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JdEzC5grp9ew"
   },
   "source": [
    "## Optimization of a Keras model using more Optuna's features such as pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "colab_type": "code",
    "id": "LfV6iONwVujP",
    "outputId": "02973112-40be-45f5-e614-607572e44862"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-01 15:43:51.160028] [OptKeras] Ready for optimization. (message printed as verbose is set to 1+)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0801 15:43:51.506155 139680493655872 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0801 15:44:23.189004 139680493655872 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "study_name = dataset_name + '_Optimized'\n",
    "\n",
    "ok = OptKeras( \n",
    "    # parameters for optuna.create_study\n",
    "    storage='sqlite:///' + study_name + '_Optuna.db', \n",
    "    sampler=optuna.samplers.TPESampler(\n",
    "        consider_prior=True, prior_weight=1.0, \n",
    "        consider_magic_clip=True, consider_endpoints=False, \n",
    "        n_startup_trials=10, n_ei_candidates=24, \n",
    "        seed=None), \n",
    "    pruner=optuna.pruners.SuccessiveHalvingPruner(\n",
    "        min_resource=1, reduction_factor=4, min_early_stopping_rate=0), \n",
    "    study_name=study_name,\n",
    "    load_if_exists=True,\n",
    "    # parameters for OptKeras\n",
    "    monitor='val_acc',\n",
    "    direction='maximize',\n",
    "    enable_pruning=True, \n",
    "    models_to_keep=1, # Either 1, 0, or -1 (save all models) \n",
    "    verbose=1,\n",
    "    )\n",
    "\n",
    "def objective(trial): \n",
    "    epochs = 2\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    if trial.suggest_int('Conv', 0, 1):  \n",
    "        # 1 Convolution layer\n",
    "        i = 1\n",
    "        model.add(Conv2D(\n",
    "            filters=int(trial.suggest_discrete_uniform(\n",
    "                'Conv_{}_num_filters'.format(i), 32, 64, 32)), \n",
    "            kernel_size=tuple([trial.suggest_int(\n",
    "                'Conv_{}_kernel_size'.format(i), 2, 3)] * 2),\n",
    "            activation='relu',\n",
    "            input_shape=input_shape))\n",
    "        model.add(MaxPooling2D(pool_size=tuple([trial.suggest_int(\n",
    "                'Conv_{}_max_pooling_size'.format(i), 2, 3)] * 2)))\n",
    "        model.add(Dropout(trial.suggest_discrete_uniform(\n",
    "                'Conv_{}_dropout_rate'.format(i), 0, 0.5, 0.25) ))\n",
    "        model.add(Flatten())        \n",
    "    else:\n",
    "        model.add(Flatten(input_shape=input_shape))\n",
    "    # 2 Fully connected layers\n",
    "    for i in np.arange(2) + 1:\n",
    "        model.add(Dense(int(trial.suggest_discrete_uniform(\n",
    "            'FC_{}_num_hidden_units'.format(i), 256, 512, 256))))\n",
    "        if trial.suggest_int('FC_{}_batch_normalization'.format(i), 0, 1):\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(Activation(trial.suggest_categorical(\n",
    "            'FC_{}_acivation'.format(i), ['relu'])))\n",
    "        model.add(Dropout(\n",
    "            trial.suggest_discrete_uniform(\n",
    "                'FC_{}_dropout_rate'.format(i), 0, 0.5, 0.25) ))\n",
    "        \n",
    "    # Output layer    \n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    optimizer_dict = { \\\n",
    "    #'Adagrad': Adagrad(),\n",
    "    'Adam': Adam() }\n",
    "    \n",
    "    model.compile(optimizer = optimizer_dict[\n",
    "        trial.suggest_categorical('Optimizer', list(optimizer_dict.keys()))],\n",
    "          loss='sparse_categorical_crossentropy', metrics=['accuracy'])    \n",
    "    \n",
    "    if ok.verbose >= 2: model.summary()\n",
    "    \n",
    "    batch_size = trial.suggest_int('Batch_size', 256, 256) \n",
    "    data_augmentation = trial.suggest_int('Data_augmentation', 0, 1)\n",
    "    \n",
    "    if not data_augmentation:\n",
    "        # [Required] Specify callbacks(trial) in fit method\n",
    "        model.fit(x_train, y_train, batch_size=batch_size,\n",
    "                  epochs=epochs, validation_data=(x_test, y_test),\n",
    "                  shuffle=True,\n",
    "                  callbacks=ok.callbacks(trial),\n",
    "                  verbose=ok.keras_verbose )\n",
    "    \n",
    "    if data_augmentation:\n",
    "        # This will do preprocessing and realtime data augmentation:\n",
    "        datagen = ImageDataGenerator(\n",
    "            width_shift_range=[-1, 0, +1], # 1 pixel\n",
    "            height_shift_range=[-1, 0, +1], # 1 pixel\n",
    "            zoom_range=[0.95,1.05],  # set range for random zoom\n",
    "            horizontal_flip=False,  # disable horizontal flip\n",
    "            vertical_flip=False )  # disable vertical flip\n",
    "        datagen.fit(x_train)\n",
    "        # [Required] Specify callbacks(trial) in fit_generator method\n",
    "        model.fit_generator(datagen.flow(x_train, y_train, \n",
    "                                         batch_size=batch_size),\n",
    "                            epochs=epochs, validation_data=(x_test, y_test),\n",
    "                            steps_per_epoch=len(x_train) // batch_size,\n",
    "                            callbacks=ok.callbacks(trial),\n",
    "                            verbose=ok.keras_verbose )\n",
    "    \n",
    "    # [Required] return trial_best_value (recommended) or latest_value\n",
    "    return ok.trial_best_value\n",
    "\n",
    "# Set n_trials and/or timeout (in sec) for optimization by Optuna\n",
    "ok.optimize(objective, timeout=60) # Run for 1 minute for demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xio3n-DJ7-F3"
   },
   "source": [
    "## Randomized Grid Search of a simple Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "colab_type": "code",
    "id": "hCarYfv175hC",
    "outputId": "0aaab45d-67ab-462a-d971-b4e0cab639da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-01 15:46:20.279641] [OptKeras] Ready for optimization. (message printed as verbose is set to 1+)\n",
      "[2019-08-01 15:46:20.376395]  (None) \n",
      "[2019-08-01 15:46:54.680899] Completed:  12% (    1 /     8)\n",
      "[2019-08-01 15:46:54.783971] Trial#: 0, value: 1.156144e-01| Best trial#: 0, value: 1.156144e-01, params: {'filters': 32, 'kernel_size': 5, 'strides': 1, 'activation': 'relu'}\n",
      "[2019-08-01 15:47:26.458536] Completed:  25% (    2 /     8)\n",
      "[2019-08-01 15:47:26.580928] Trial#: 1, value: 2.880034e-01| Best trial#: 0, value: 1.156144e-01, params: {'filters': 32, 'kernel_size': 5, 'strides': 1, 'activation': 'relu'}\n",
      "[2019-08-01 15:47:55.503160] Completed:  38% (    3 /     8)\n",
      "[2019-08-01 15:47:55.644526] Trial#: 2, value: 2.756525e-01| Best trial#: 0, value: 1.156144e-01, params: {'filters': 32, 'kernel_size': 5, 'strides': 1, 'activation': 'relu'}\n",
      "[2019-08-01 15:48:56.419806] Completed:  50% (    4 /     8)\n",
      "[2019-08-01 15:48:56.579812] Trial#: 3, value: 2.845126e-01| Best trial#: 0, value: 1.156144e-01, params: {'filters': 32, 'kernel_size': 5, 'strides': 1, 'activation': 'relu'}\n",
      "[2019-08-01 15:49:58.059236] Completed:  62% (    5 /     8)\n",
      "[2019-08-01 15:49:58.176409] Trial#: 4, value: 6.919648e-02| Best trial#: 4, value: 6.919648e-02, params: {'filters': 64, 'kernel_size': 5, 'strides': 1, 'activation': 'relu'}\n",
      "[2019-08-01 15:51:32.329431] Completed:  75% (    6 /     8)\n",
      "[2019-08-01 15:51:32.453828] Trial#: 7, value: 1.031057e-01| Best trial#: 4, value: 6.919648e-02, params: {'filters': 64, 'kernel_size': 5, 'strides': 1, 'activation': 'relu'}\n",
      "[2019-08-01 15:52:53.550962] Completed:  88% (    7 /     8)\n",
      "[2019-08-01 15:52:53.684188] Trial#: 9, value: 2.765517e-01| Best trial#: 4, value: 6.919648e-02, params: {'filters': 64, 'kernel_size': 5, 'strides': 1, 'activation': 'relu'}\n",
      "[2019-08-01 15:56:29.264115] Completed: 100% (    8 /     8)\n",
      "[2019-08-01 15:56:29.303976] Trial#: 18, value: 1.297754e-01| Best trial#: 4, value: 6.919648e-02, params: {'filters': 64, 'kernel_size': 5, 'strides': 1, 'activation': 'relu'}\n"
     ]
    }
   ],
   "source": [
    "study_name = dataset_name + '_GridSearch'\n",
    "\n",
    "\"\"\" To run randomized grid search, set random_grid_search_mode True \"\"\"\n",
    "ok = OptKeras(study_name=study_name, random_grid_search_mode=True)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(\n",
    "        filters=trial.suggest_categorical('filters', [32, 64]),\n",
    "        kernel_size=trial.suggest_categorical('kernel_size', [3, 5]),\n",
    "        strides=trial.suggest_categorical('strides', [1]),\n",
    "        activation=trial.suggest_categorical('activation', ['relu', 'linear']),\n",
    "        input_shape=input_shape))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer=Adam(),\n",
    "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              validation_data=(x_test, y_test), shuffle=True,\n",
    "              batch_size=512, epochs=2,\n",
    "              callbacks=ok.callbacks(trial),\n",
    "              verbose=ok.keras_verbose)\n",
    "\n",
    "    return ok.trial_best_value\n",
    "\n",
    "\"\"\" Set the number of parameter sets as n_trials for complete grid search \"\"\"\n",
    "ok.random_grid_search(objective, n_trials=2*2*2)  # 2*2*2 = 8 param sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "rYKEKdiP6f6v",
    "outputId": "8c109d93-0ab9-4767-f3f1-67bd69c0fc75"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ## The end of code. '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" ## The end of code. \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Of83kdUpkmb"
   },
   "source": [
    "Please feel free to post questions or feedback [here](\n",
    "https://github.com/Minyus/optkeras/issues\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "OptKeras_Example.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
